{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10667608,"sourceType":"datasetVersion","datasetId":6606744}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\nimport logging\nfrom datetime import datetime\nimport os\n\n# Constants for configuration\nLOG_FILE_PATH = '/kaggle/working/data_loading.log'\nINPUT_FILE_PATH = '/kaggle/input/all-amazon-review/All_Amazon_Review.json'\nOUTPUT_FILE_PATH = '/kaggle/working/processed_data.parquet'\nSPARK_APP_NAME = \"Amazon Review Data Loading\"\nSPARK_MEMORY_CONFIG = {\n    \"spark.driver.memory\": \"8g\",\n    \"spark.executor.memory\": \"8g\",\n    \"spark.sql.shuffle.partitions\": \"200\",\n    \"spark.memory.fraction\": \"0.8\"\n}\n\n# Configure logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\nif os.access('/kaggle/working/', os.W_OK):\n    file_handler = logging.FileHandler(LOG_FILE_PATH, mode='w')\n    file_handler.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    print(f\"Logging configured successfully. Log file: {LOG_FILE_PATH}\")\n\n# Stop any existing Spark session\ntry:\n    spark = SparkSession.builder.getOrCreate()\n    spark.stop()\n    logging.info(\"Stopped existing Spark session.\")\n    print(\"Stopped existing Spark session.\")\nexcept Exception as e:\n    logging.warning(f\"No active Spark session found or error stopping session: {e}\")\n    print(f\"No active Spark session found or error stopping session: {e}\")\n\n# Initialize Spark Session\ntry:\n    spark_builder = SparkSession.builder.appName(SPARK_APP_NAME)\n    \n    # Set Spark configurations\n    for key, value in SPARK_MEMORY_CONFIG.items():\n        spark_builder.config(key, value)\n    \n    # Enable GPU if available\n    spark_builder.config(\"spark.executor.resource.gpu.amount\", \"1\")\n    spark_builder.config(\"spark.task.resource.gpu.amount\", \"1\")\n    spark_builder.config(\"spark.executor.resource.gpu.discoveryScript\", \"/opt/sparkRapidsPlugin/getGpusResources.sh\")\n    \n    spark = spark_builder.getOrCreate()\n    logging.info(\"Spark session started successfully.\")\n    print(\"Spark session started successfully.\")\n\n    # Define schema for optimization\n    schema = StructType([\n        StructField(\"reviewerID\", StringType(), True),\n        StructField(\"asin\", StringType(), True),\n        StructField(\"reviewText\", StringType(), True),\n        StructField(\"overall\", IntegerType(), True),\n        StructField(\"unixReviewTime\", LongType(), True)\n    ])\n\n    # Start data loading\n    start_time = datetime.now()\n    logging.info(\"Data loading started.\")\n    print(\"Data loading started.\")\n\n    # Read gzipped JSON file with schema\n    df = spark.read.json(INPUT_FILE_PATH, schema=schema, mode=\"PERMISSIVE\")\n    logging.info(\"Data read successfully.\")\n    print(\"Data read successfully.\")\n\n    # Optimize partitions for memory\n    df = df.coalesce(50)  # Reduce partitions to avoid memory overload\n    logging.info(\"Data repartitioned successfully.\")\n    print(\"Data repartitioned successfully.\")\n\n    # Display only a few records to avoid crashing\n    logging.info(\"Displaying sample data...\")\n    print(\"Displaying sample data...\")\n    df.limit(10).show(truncate=False)  # Reduce to 10 rows\n\n    # Approximate record count without full-table scan\n    logging.info(\"Displaying sample record count...\")\n    print(f\"Sample records loaded: {df.take(10)}\")\n\n    # Check for missing values efficiently\n    from pyspark.sql.functions import col, count, when\n    logging.info(\"Checking for missing values...\")\n    print(\"Checking for missing values...\")\n    null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n    null_counts.show()\n\n    # Save processed data in Parquet format (overwrite mode)\n    logging.info(\"Saving processed data as Parquet...\")\n    print(\"Saving processed data as Parquet...\")\n    df.write.mode(\"overwrite\").parquet(OUTPUT_FILE_PATH)\n    logging.info(\"Data saved successfully.\")\n    print(\"Data saved successfully.\")\n\n    end_time = datetime.now()\n    logging.info(f\"Data loading completed in {end_time - start_time}.\")\n    print(f\"Data loading completed in {end_time - start_time}.\")\n\nexcept Exception as e:\n    logging.error(f\"Error during data loading: {e}\")\n    print(f\"Error occurred: {e}\")\n\nfinally:\n    if 'spark' in locals():\n        spark.stop()\n        logging.info(\"Spark session stopped.\")\n        print(\"Spark session stopped.\")\n    # Ensure logs are flushed\n    for handler in logger.handlers:\n        handler.flush()\n        handler.close()\n    logger.removeHandler(file_handler)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T13:47:37.140021Z","iopub.execute_input":"2025-02-05T13:47:37.140331Z"}},"outputs":[{"name":"stdout","text":"Logging configured successfully. Log file: /kaggle/working/data_loading.log\nStopped existing Spark session.\nSpark session started successfully.\nData loading started.\nData read successfully.\nData repartitioned successfully.\nDisplaying sample data...\n","output_type":"stream"}],"execution_count":null}]}